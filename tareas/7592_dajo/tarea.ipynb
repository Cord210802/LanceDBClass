{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drlecter/.pyenv/versions/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import lancedb\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este ejercicio descargamos una base de datos de mensajes de texto en inglés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"chirunder/text_messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top right I gained a little speed with the add...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>They are heavier wheels though as are all the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Federally registering a trademark is more than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'll have to jog my memory from rooting a few ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Unless you can afford to buy all new larger cl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texto\n",
       "0  Top right I gained a little speed with the add...\n",
       "1  They are heavier wheels though as are all the ...\n",
       "2  Federally registering a trademark is more than...\n",
       "3  I'll have to jog my memory from rooting a few ...\n",
       "4  Unless you can afford to buy all new larger cl..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(dataset['train'])\n",
    "df.rename(columns={'text': 'texto'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para los siguientes ejercicios, voy a crear una variable del numero de palabras en cada mensaje de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>texto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>Top right I gained a little speed with the add...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>They are heavier wheels though as are all the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>Federally registering a trademark is more than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>I'll have to jog my memory from rooting a few ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>Unless you can afford to buy all new larger cl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n                                              texto\n",
       "0  13  Top right I gained a little speed with the add...\n",
       "1  14  They are heavier wheels though as are all the ...\n",
       "2   9  Federally registering a trademark is more than...\n",
       "3  21  I'll have to jog my memory from rooting a few ...\n",
       "4  10  Unless you can afford to buy all new larger cl..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['n'] = df['texto'].apply(lambda x: len(str(x).split()))\n",
    "df = df[['n', 'texto']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1:\n",
    "A partir del dataframe df, crea df_tokenized (usando el Tokenizer de GPT2) con dos columnas pero con el texto tokenizado. Asegurate de que todos los embeddings tengan la misma longitud y los tokens sean enteros (todos enteros o todos doubles). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame size: (11615290, 2)\n",
      "Index(['vector', 'texto', 'n'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Cargar el tokenizer del modelo GPT2 de Hugging Face\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Definir una función para tokenizar y ajustar la longitud a 300 tokens\n",
    "def tokenize_text(text):\n",
    "    try:\n",
    "        tokens = tokenizer.encode(text, truncation=True, padding='max_length', max_length=300)\n",
    "        return tokens\n",
    "    except Exception as e:\n",
    "        print(f\"Error tokenizing text: {text}\\n{e}\")\n",
    "        return [tokenizer.pad_token_id] * 300  # Return a padded token sequence in case of error\n",
    "\n",
    "# Verificar el tamaño del DataFrame\n",
    "print(f\"DataFrame size: {df.shape}\")\n",
    "\n",
    "# Trabajar con una muestra si el DataFrame es muy grande\n",
    "df_sample = df.sample(n=1000, random_state=42) if len(df) > 1000 else df\n",
    "\n",
    "# Aplicar la tokenización a la columna 'texto' del DataFrame\n",
    "df_sample['tokenized_text'] = df_sample['texto'].apply(tokenize_text)\n",
    "\n",
    "# Crear el nuevo DataFrame con las columnas requeridas\n",
    "df_tokenized = df_sample[['tokenized_text', 'texto', 'n']].copy()\n",
    "df_tokenized.columns = ['vector', 'texto', 'n']\n",
    "\n",
    "print(df_tokenized.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2:\n",
    "Mete el dataframe a una tabla en una base de datos de LanceDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow.Table\n",
      "vector: fixed_size_list<item: float>[300]\n",
      "  child 0, item: float\n",
      "texto: string\n",
      "n: int64\n",
      "----\n",
      "vector: [[[56,929,11,326,338,...,50256,50256,50256,50256,50256],[40,4030,616,5852,37976,...,50256,50256,50256,50256,50256],[21352,1660,318,635,1049,...,50256,50256,50256,50256,50256],[1026,318,407,3306,284,...,50256,50256,50256,50256,50256],[9690,287,5963,355,314,...,50256,50256,50256,50256,50256]]]\n",
      "texto: [[\"Yup, that's another, and there are several others that are really the same as far as I can tell.\",\"I kept my seatbelt on.\",\"Hot water is also great to have for hypothermia, emergency medicine sterilization, etc.\",\"It is not necessary to slather the bolt in it.\",\"Thanks in advance as I'm sure I'll receive an answer or be directed to another post from the past.\"]]\n",
      "n: [[19,5,13,10,19]]\n"
     ]
    }
   ],
   "source": [
    "# Resuelve Conectar a una base de datos local de LanceDB\n",
    "database = lancedb.connect(\"./.lancedb\")\n",
    "\n",
    "# Crear una tabla en la base de datos con el DataFrame tokenizado\n",
    "database.create_table(\"tabla_tokenizada\", df_tokenized)\n",
    "\n",
    "# Mostrar las primeras filas de la tabla creada para verificar\n",
    "print(database[\"tabla_tokenizada\"].head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3:\n",
    "Haz una query estilo SQL a la tabla de la base de datos. Quiero que escribas la query equivalente y pongas la explicación de lo que está haciendo la consulta. Hint: usa la columna \"n\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             texto  \\\n",
      "0           I kept my seatbelt on.   \n",
      "1  How about getting a girlfriend.   \n",
      "2        I'm wheeling it tomorrow.   \n",
      "3                    thanks frick.   \n",
      "4                       Andrew No.   \n",
      "5       I can't believe its gonna.   \n",
      "6                Even on Hardcore.   \n",
      "7                    Yah found it.   \n",
      "8                  Water is Clear.   \n",
      "9        Yeah, I wasnt that stuck.   \n",
      "\n",
      "                                              vector  \n",
      "0  [40.0, 4030.0, 616.0, 5852.0, 37976.0, 319.0, ...  \n",
      "1  [2437.0, 546.0, 1972.0, 257.0, 11077.0, 13.0, ...  \n",
      "2  [40.0, 1101.0, 7825.0, 278.0, 340.0, 9439.0, 1...  \n",
      "3  [27547.0, 1216.0, 624.0, 13.0, 50256.0, 50256....  \n",
      "4  [20508.0, 1400.0, 13.0, 50256.0, 50256.0, 5025...  \n",
      "5  [40.0, 460.0, 470.0, 1975.0, 663.0, 8066.0, 13...  \n",
      "6  [6104.0, 319.0, 49691.0, 13.0, 50256.0, 50256....  \n",
      "7  [56.0, 993.0, 1043.0, 340.0, 13.0, 50256.0, 50...  \n",
      "8  [19184.0, 318.0, 11459.0, 13.0, 50256.0, 50256...  \n",
      "9  [10995.0, 11.0, 314.0, 373.0, 429.0, 326.0, 78...  \n"
     ]
    }
   ],
   "source": [
    "# Resuelve el task 3 aqui# Realizar una consulta a la tabla en la base de datos\n",
    "result_df = (\n",
    "    database[\"tabla_tokenizada\"].search()\n",
    "    .where(\"n <= 5\")  # Condición: seleccionar filas donde \"n\" es menor o igual a 5\n",
    "    .select([\"texto\", \"vector\"])  # Seleccionar las columnas \"texto\" y \"vector\"\n",
    "    .limit(10)  # Limitar los resultados a 11 filas\n",
    "    .to_pandas()  # Convertir el resultado a un DataFrame de pandas\n",
    ")\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Query en SQL equivalente:\n",
    "- Explicacion: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4:\n",
    "Inventa un mensaje de texto que tu podrías escribirle a un amigo. Tokenizalo y ponlo en el formato adecuado para hacer un vector query. Quiero que me regreses el mensaje más parecido al mensaje que inventaste (OJO: quiero el texto, no el embedding). HINT: Hay que decodear el resultado del query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Trabajar con una muestra si el DataFrame es muy grande\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m df_tokenized\u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39msample(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mdf\u001b[49m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m500\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m df\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Aplicar la tokenización a la columna 'texto' del DataFrame\u001b[39;00m\n\u001b[1;32m     20\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenized_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtexto\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(tokenize_text)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Cargar el tokenizer del modelo GPT2 de Hugging Face\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Inventar un mensaje de texto\n",
    "mensaje_amigo = \"Hey, ¿qué planes tienes para el fin de semana? Me gustaría salir a caminar.\"\n",
    "\n",
    "# Tokenizar el mensaje\n",
    "mensaje_tokenizado = tokenizer.encode(mensaje_amigo, truncation=True, padding='max_length', max_length=300)\n",
    "\n",
    "# Definir una función para tokenizar y ajustar la longitud a 300 tokens\n",
    "def tokenize_text(text):\n",
    "    tokens = tokenizer.encode(text, truncation=True, padding='max_length', max_length=300)\n",
    "    return tokens\n",
    "\n",
    "# Aplicar la tokenización a la columna 'texto' del DataFrame\n",
    "df['tokenized_text'] = df['texto'].apply(tokenize_text)\n",
    "\n",
    "# Crear el nuevo DataFrame con las columnas requeridas\n",
    "df_tokenized = df[['tokenized_text', 'texto', 'n']].copy()\n",
    "df_tokenized.columns = ['vector', 'texto', 'n']\n",
    "\n",
    "# Conectar a una base de datos local de LanceDB\n",
    "database = lancedb.connect(\"./.lancedb\")\n",
    "\n",
    "# Crear una tabla en la base de datos con el DataFrame tokenizado\n",
    "database.create_table(\"tabla_tokenizada\", df_tokenized)\n",
    "\n",
    "# Mostrar las primeras filas de la tabla creada para verificar\n",
    "print(database[\"tabla_tokenizada\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar una consulta a la tabla en la base de datos usando el vector del mensaje tokenizado\n",
    "result = (\n",
    "    database[\"tabla_tokenizada\"].search()\n",
    "    .where(\"n <= 5\")  # Ejemplo de condición para la consulta\n",
    "    .select([\"texto\", \"vector\"])  # Seleccionar las columnas \"texto\" y \"vector\"\n",
    "    .limit(11)  # Limitar los resultados a 11 filas\n",
    "    .to_pandas()  # Convertir el resultado a un DataFrame de pandas\n",
    ")\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar una consulta vectorial\n",
    "similar_result = (\n",
    "    database[\"tabla_tokenizada\"].search()\n",
    "    .vector_search(mensaje_tokenizado)  # Realizar la búsqueda vectorial con el mensaje tokenizado\n",
    "    .select([\"texto\"])  # Seleccionar solo la columna \"texto\"\n",
    "    .limit(1)  # Limitar el resultado a la fila más parecida\n",
    "    .to_pandas()  # Convertir el resultado a un DataFrame de pandas\n",
    ")\n",
    "\n",
    "# Decodificar el resultado\n",
    "mensaje_mas_parecido = similar_result['texto'].iloc[0]\n",
    "\n",
    "# Mostrar el mensaje más parecido\n",
    "print(f\"Mensaje más parecido: {mensaje_mas_parecido}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lance_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
